{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4106d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\99455\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\99455\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\99455\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\99455\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\99455\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\99455\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa0dde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n",
    "video_path = \"C:\\\\Users\\\\99455\\\\Desktop\\\\DataScience\\\\Proctoring Task\\\\testformodel.mp4\"\n",
    "#video_path =  'C:/Users/99455/Desktop/DataScience/OEP database/subject1/Yousef1.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a27e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_face_mesh = mp.solutions.face_mesh\n",
    "#face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "face_mesh = mp.solutions.face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) # confidence threshold\n",
    "#)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1,circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b3ead49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n",
      "face not detected\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    start = time.time()\n",
    "    \n",
    "    image = cv2.cvtColor(cv2.flip(image,1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    results = face_mesh.process(image)\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    rdet = face_detection.process(image)\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "    if not rdet.detections:\n",
    "        print(\"face not detected\")\n",
    "    else:\n",
    "        \n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x*img_w, lm.y*img_h)  \n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "                        \n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "                    face_2d.append([x, y])\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "                        \n",
    "            face_2d = np.array(face_2d,dtype=np.float64)\n",
    "            \n",
    "            face_3d = np.array(face_3d,dtype=np.float64)\n",
    "            \n",
    "            focal_length = 1 * img_w\n",
    "            \n",
    "            cam_matrix = np.array([ [focal_length, 0 ,img_h/2], [0, focal_length, img_w/2], [0,0,1] ])\n",
    "            \n",
    "            dist_matrix = np.zeros((4,1),dtype=np.float64)\n",
    "            \n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            \n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "            \n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "            \n",
    "            x = angles[0]*360\n",
    "            y = angles[1]*360\n",
    "            z = angles[2]*360\n",
    "            \n",
    "            if y < -10:\n",
    "                text = \"Looking left\"\n",
    "            elif y > 10:\n",
    "                text = \"Looking right\"\n",
    "            elif x < -10:\n",
    "                text = \"Looking down\"\n",
    "            elif x > 10:\n",
    "                text = \"Looking up\"\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "            \n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec,trans_vec, cam_matrix, dist_matrix)\n",
    "            \n",
    "            p1 =(int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 =(int(nose_2d[0]+y*10), int(nose_2d[1]-x*10))\n",
    "            \n",
    "            cv2.line(image,p1,p2, (255,0,0),3)\n",
    "            \n",
    "            cv2.putText(image,text,(20,50),cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0),2)\n",
    "            cv2.putText(image,\"x: \"+str(np.round(x,2)), (500,50), cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255))\n",
    "            cv2.putText(image,\"y: \"+str(np.round(y,2)), (500,100), cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255))\n",
    "            cv2.putText(image,\"z: \"+str(np.round(z,2)), (500,150), cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255))\n",
    "            \n",
    "        end = time.time() \n",
    "        #totalTime = end - start\n",
    "        #print(end)\n",
    "        #print(start)\n",
    "        #fps = 1/(totalTime) # +0.01)\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        #print(\"FPS: \", fps)\n",
    "        cv2.putText(image,f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0),2)\n",
    "        \n",
    "        #mp_drawing.draw_landmarks(image=image, landmark_list = face_landmarks, connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        #                         landmark_drawing_spec =drawing_spec, connection_drawing_spec=drawing_spec)\n",
    "        \n",
    "        cv2.imshow(\"Head Pose Estimation\", image)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "img1 =  cv2.imread('C:/Users/99455/Desktop/test photos/test111.jpg')\n",
    "img2 =  cv2.imread('C:/Users/99455/Desktop/test photos/test222.jpg')\n",
    "np_img1 = np.array(img1)\n",
    "np_img2 = np.array(img2)\n",
    "\n",
    "\n",
    "#cosine_sim = np.dot(img1, img2) / (norm(img1) * norm(img2))\n",
    "Aflat = np_img1.flatten()\n",
    "Bflat = np_img2.flatten()\n",
    "\n",
    "dist =  1 - distance.cosine(Aflat, Bflat)\n",
    "#cosine_sim = np.dot(Aflat, Bflat) / (norm(Aflat) * norm(Bflat))\n",
    "#print(cosine_sim)\n",
    "print(dist)\n",
    "#bv = video_size_threshold(video)\n",
    "#print(bv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Load images\n",
    "#image1 = cv2.imread(image1)\n",
    "#image2 = cv2.imread(image2)\n",
    "hist_img1 = cv2.calcHist([image1], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img1[255, 255, 255] = 0 #ignore all white pixels\n",
    "cv2.normalize(hist_img1, hist_img1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "hist_img2 = cv2.calcHist([image2], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img2[255, 255, 255] = 0  #ignore all white pixels\n",
    "cv2.normalize(hist_img2, hist_img2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "# Find the metric value\n",
    "metric_val = cv2.compareHist(hist_img1, hist_img2, cv2.HISTCMP_CORREL)\n",
    "print(f\"Similarity Score: \", round(metric_val, 2))\n",
    "# Similarity Score: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36007ef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m rect_start_point \u001b[38;5;241m=\u001b[39m _normalized_to_pixel_coordinates(relative_bounding_box\u001b[38;5;241m.\u001b[39mxmin, relative_bounding_box\u001b[38;5;241m.\u001b[39mymin, image_cols,image_rows)\n\u001b[0;32m     36\u001b[0m rect_end_point \u001b[38;5;241m=\u001b[39m _normalized_to_pixel_coordinates(relative_bounding_box\u001b[38;5;241m.\u001b[39mxmin \u001b[38;5;241m+\u001b[39m relative_bounding_box\u001b[38;5;241m.\u001b[39mwidth, relative_bounding_box\u001b[38;5;241m.\u001b[39mymin \u001b[38;5;241m+\u001b[39m relative_bounding_box\u001b[38;5;241m.\u001b[39mheight, image_cols, image_rows)\n\u001b[1;32m---> 38\u001b[0m xleft,ytop \u001b[38;5;241m=\u001b[39m rect_start_point\n\u001b[0;32m     39\u001b[0m xright,ybot \u001b[38;5;241m=\u001b[39m rect_end_point\n\u001b[0;32m     40\u001b[0m crop_img \u001b[38;5;241m=\u001b[39m image_input[ytop: ybot, xleft: xright]\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(video_path)\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "#start_frame_number +=15\n",
    "#cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame_number)\n",
    "\n",
    "frame_no = 0  \n",
    "processed_frame_count = 0  \n",
    "\n",
    "total_grab_time = 0 \n",
    "total_retrieve_time = 0 \n",
    "\n",
    "start = time.time()  \n",
    "skip_rate = 10\n",
    "\n",
    "while True:\n",
    "    tmp = time.time()\n",
    "    #ret = video.grab()\n",
    "    ret, frame = video.read()\n",
    "    total_grab_time += (time.time() - tmp)\n",
    "    \n",
    "    if not ret:\n",
    "        break  # Video ended\n",
    "    frame_no += 1\n",
    "\n",
    "    if (frame_no % skip_rate == 0):  # Processing frame\n",
    "        processed_frame_count += 1\n",
    "        image_rows, image_cols, _ = frame.shape\n",
    "        image_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_input)\n",
    "        if results.detections:\n",
    "            detection=results.detections[0]\n",
    "            location = detection.location_data\n",
    "            relative_bounding_box = location.relative_bounding_box\n",
    "            \n",
    "            rect_start_point = _normalized_to_pixel_coordinates(relative_bounding_box.xmin, relative_bounding_box.ymin, image_cols,image_rows)\n",
    "            rect_end_point = _normalized_to_pixel_coordinates(relative_bounding_box.xmin + relative_bounding_box.width, relative_bounding_box.ymin + relative_bounding_box.height, image_cols, image_rows)\n",
    "\n",
    "            xleft,ytop = rect_start_point\n",
    "            xright,ybot = rect_end_point\n",
    "            crop_img = image_input[ytop: ybot, xleft: xright]\n",
    "            image_input = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(\"Frames/frame%d.jpg\" % frame_no, crop_img) \n",
    "            \n",
    "            \n",
    "        #tmp = time.time()\n",
    "        #status, frame = video.retrieve()  # Decode processing frame\n",
    "        #total_retrieve_time += (time.time() - tmp)\n",
    "        \n",
    "        \n",
    "        # Some image processing operations; resize, convert to gray scale and show frame\n",
    "        #frame = cv2.resize(frame, (1280,720))\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #cv2.imshow(\"Frame\", frame)\n",
    "        #success, image = vidObj.read() \n",
    "        \n",
    "  \n",
    "        # Saves the frames with frame-count \n",
    "            \n",
    "        #cv2.waitKey(1)  # Wait for 1 ms, this is to clearly see the frame on screen\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cd667c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmin: 0.28152302\n",
      "ymin: 0.03002569\n",
      "width: 0.30073914\n",
      "height: 0.53456485\n",
      "\n",
      "5430\n",
      "0.6322621583938599\n",
      "0.6645905435085296\n"
     ]
    }
   ],
   "source": [
    "print(location.relative_bounding_box)\n",
    "print(frame_no)\n",
    "print(relative_bounding_box.xmin + relative_bounding_box.width+0.05)\n",
    "print(relative_bounding_box.ymin + relative_bounding_box.height+0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "\n",
    "    \n",
    "    #image = cv2.cvtColor(cv2.flip(image,1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    results = face_mesh.process(image)\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    img_h, img_w, img_c = image.shape\n",
    "    np_img = np.array(image)\n",
    "    print(np_img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
